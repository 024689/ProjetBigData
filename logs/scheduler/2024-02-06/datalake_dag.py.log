[2024-02-06T21:27:26.449+0100] {processor.py:161} INFO - Started process (PID=23034) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:27:26.462+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:27:26.478+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:27:26.470+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:27:26.746+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:27:26.855+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:27:26.853+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:27:27.167+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:27:27.167+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:27:27.240+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.885 seconds
[2024-02-06T21:28:44.104+0100] {processor.py:161} INFO - Started process (PID=23997) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:28:44.108+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:28:44.110+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:28:44.109+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:28:44.143+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:28:44.374+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:28:44.374+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:28:44.420+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:28:44.419+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:28:44.480+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.383 seconds
[2024-02-06T21:29:28.754+0100] {processor.py:161} INFO - Started process (PID=24374) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:29:28.760+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:29:28.767+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:29:28.765+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:29:28.837+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:29:28.901+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:29:28.901+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:29:28.993+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:29:28.992+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:29:29.189+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.442 seconds
[2024-02-06T21:30:06.856+0100] {processor.py:161} INFO - Started process (PID=24604) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:30:06.859+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:30:06.864+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:30:06.863+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:30:06.899+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:30:06.938+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:30:06.938+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:30:06.976+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:30:06.976+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:30:07.148+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.296 seconds
[2024-02-06T21:30:34.553+0100] {processor.py:161} INFO - Started process (PID=24794) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:30:34.558+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:30:34.564+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:30:34.561+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:30:34.662+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:30:34.747+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:30:34.746+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:30:34.805+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:30:34.804+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:30:34.936+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.397 seconds
[2024-02-06T21:31:45.828+0100] {processor.py:161} INFO - Started process (PID=25090) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:31:45.834+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:31:45.837+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:31:45.836+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:31:45.898+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:31:45.942+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:31:45.941+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:31:46.013+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:31:46.013+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:31:46.090+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.275 seconds
[2024-02-06T21:32:22.659+0100] {processor.py:161} INFO - Started process (PID=25142) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:32:22.662+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:32:22.669+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:32:22.668+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:32:22.700+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:32:22.727+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:32:22.726+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:32:22.768+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:32:22.767+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:32:22.828+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.175 seconds
[2024-02-06T21:33:00.807+0100] {processor.py:161} INFO - Started process (PID=25316) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:33:00.812+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:33:00.814+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:33:00.813+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:33:00.845+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:33:00.876+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:33:00.875+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:33:00.944+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:33:00.943+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:33:01.077+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.276 seconds
[2024-02-06T21:33:42.893+0100] {processor.py:161} INFO - Started process (PID=25604) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:33:42.898+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:33:42.901+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:33:42.900+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:33:43.003+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:33:43.088+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:33:43.061+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:33:43.089+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:33:43.089+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:33:43.221+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:33:43.221+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:33:43.721+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.835 seconds
[2024-02-06T21:34:14.069+0100] {processor.py:161} INFO - Started process (PID=25786) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:34:14.071+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:34:14.074+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:34:14.074+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:34:14.150+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:34:14.179+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:34:14.175+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:34:14.186+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:34:14.180+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:34:14.317+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:34:14.317+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:34:14.476+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.419 seconds
[2024-02-06T21:34:45.417+0100] {processor.py:161} INFO - Started process (PID=25892) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:34:45.423+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:34:45.425+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:34:45.425+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:34:45.436+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:34:45.433+0100] {dagbag.py:348} ERROR - Failed to import: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/airflow/dags/datalake_dag.py", line 7, in <module>
    from extraction import extraction_from_api_and_save_to_hdfs
  File "/home/ubuntu/airflow/extraction.py", line 3, in <module>
    from utils.api_utils import get_data_from_api_and_save_to_hdfs, get_data_from_api, \
  File "/home/ubuntu/airflow/utils/api_utils.py", line 2, in <module>
    from .hdfs_utils import send_file_to_hdfs
  File "/home/ubuntu/airflow/utils/hdfs_utils.py", line 1, in <module>
    from hdfs import InsecureClient
ModuleNotFoundError: No module named 'hdfs'
[2024-02-06T21:34:45.439+0100] {processor.py:842} WARNING - No viable dags retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:34:45.500+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.090 seconds
[2024-02-06T21:35:15.813+0100] {processor.py:161} INFO - Started process (PID=26229) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:35:15.818+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:35:15.822+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:35:15.821+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:35:15.834+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:35:15.833+0100] {dagbag.py:348} ERROR - Failed to import: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/airflow/dags/datalake_dag.py", line 7, in <module>
    from extraction import extraction_from_api_and_save_to_hdfs
  File "/home/ubuntu/airflow/extraction.py", line 3, in <module>
    from utils.api_utils import get_data_from_api_and_save_to_hdfs, get_data_from_api, \
  File "/home/ubuntu/airflow/utils/api_utils.py", line 2, in <module>
    from .hdfs_utils import send_file_to_hdfs
  File "/home/ubuntu/airflow/utils/hdfs_utils.py", line 1, in <module>
    from hdfs import InsecureClient
ModuleNotFoundError: No module named 'hdfs'
[2024-02-06T21:35:15.836+0100] {processor.py:842} WARNING - No viable dags retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:35:15.893+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.089 seconds
[2024-02-06T21:35:46.078+0100] {processor.py:161} INFO - Started process (PID=26372) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:35:46.084+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:35:46.087+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:35:46.085+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:35:46.119+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:35:46.111+0100] {dagbag.py:348} ERROR - Failed to import: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/airflow/dags/datalake_dag.py", line 7, in <module>
    from extraction import extraction_from_api_and_save_to_hdfs
  File "/home/ubuntu/airflow/extraction.py", line 3, in <module>
    from utils.api_utils import get_data_from_api_and_save_to_hdfs, get_data_from_api, \
  File "/home/ubuntu/airflow/utils/api_utils.py", line 2, in <module>
    from .hdfs_utils import send_file_to_hdfs
  File "/home/ubuntu/airflow/utils/hdfs_utils.py", line 1, in <module>
    from hdfs import InsecureClient
ModuleNotFoundError: No module named 'hdfs'
[2024-02-06T21:35:46.121+0100] {processor.py:842} WARNING - No viable dags retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:35:46.214+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.143 seconds
[2024-02-06T21:36:16.798+0100] {processor.py:161} INFO - Started process (PID=26479) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:36:16.808+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:36:16.813+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:36:16.812+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:36:16.825+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:36:16.823+0100] {dagbag.py:348} ERROR - Failed to import: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/airflow/dags/datalake_dag.py", line 7, in <module>
    from extraction import extraction_from_api_and_save_to_hdfs
  File "/home/ubuntu/airflow/extraction.py", line 3, in <module>
    from utils.api_utils import get_data_from_api_and_save_to_hdfs, get_data_from_api, \
  File "/home/ubuntu/airflow/utils/api_utils.py", line 2, in <module>
    from .hdfs_utils import send_file_to_hdfs
  File "/home/ubuntu/airflow/utils/hdfs_utils.py", line 1, in <module>
    from hdfs import InsecureClient
ModuleNotFoundError: No module named 'hdfs'
[2024-02-06T21:36:16.827+0100] {processor.py:842} WARNING - No viable dags retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:36:16.887+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.098 seconds
[2024-02-06T21:36:47.146+0100] {processor.py:161} INFO - Started process (PID=26586) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:36:47.148+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:36:47.150+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:36:47.149+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:36:47.161+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:36:47.160+0100] {dagbag.py:348} ERROR - Failed to import: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/airflow/dags/datalake_dag.py", line 7, in <module>
    from extraction import extraction_from_api_and_save_to_hdfs
  File "/home/ubuntu/airflow/extraction.py", line 3, in <module>
    from utils.api_utils import get_data_from_api_and_save_to_hdfs, get_data_from_api, \
  File "/home/ubuntu/airflow/utils/api_utils.py", line 2, in <module>
    from .hdfs_utils import send_file_to_hdfs
  File "/home/ubuntu/airflow/utils/hdfs_utils.py", line 1, in <module>
    from hdfs import InsecureClient
ModuleNotFoundError: No module named 'hdfs'
[2024-02-06T21:36:47.162+0100] {processor.py:842} WARNING - No viable dags retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:36:47.210+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.070 seconds
[2024-02-06T21:37:17.899+0100] {processor.py:161} INFO - Started process (PID=26657) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:37:17.908+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:37:17.912+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:37:17.910+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:37:17.981+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:37:17.928+0100] {dagbag.py:348} ERROR - Failed to import: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/airflow/dags/datalake_dag.py", line 7, in <module>
    from extraction import extraction_from_api_and_save_to_hdfs
  File "/home/ubuntu/airflow/extraction.py", line 3, in <module>
    from utils.api_utils import get_data_from_api_and_save_to_hdfs, get_data_from_api, \
  File "/home/ubuntu/airflow/utils/api_utils.py", line 2, in <module>
    from .hdfs_utils import send_file_to_hdfs
  File "/home/ubuntu/airflow/utils/hdfs_utils.py", line 1, in <module>
    from hdfs import InsecureClient
ModuleNotFoundError: No module named 'hdfs'
[2024-02-06T21:37:17.983+0100] {processor.py:842} WARNING - No viable dags retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:37:18.345+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.462 seconds
[2024-02-06T21:37:48.801+0100] {processor.py:161} INFO - Started process (PID=26752) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:37:48.808+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:37:48.811+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:37:48.810+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:37:48.874+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:37:48.871+0100] {dagbag.py:348} ERROR - Failed to import: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/airflow/dags/datalake_dag.py", line 7, in <module>
    from extraction import extraction_from_api_and_save_to_hdfs
  File "/home/ubuntu/airflow/extraction.py", line 3, in <module>
    from utils.api_utils import get_data_from_api_and_save_to_hdfs, get_data_from_api, \
  File "/home/ubuntu/airflow/utils/api_utils.py", line 2, in <module>
    from .hdfs_utils import send_file_to_hdfs
  File "/home/ubuntu/airflow/utils/hdfs_utils.py", line 1, in <module>
    from hdfs import InsecureClient
ModuleNotFoundError: No module named 'hdfs'
[2024-02-06T21:37:48.887+0100] {processor.py:842} WARNING - No viable dags retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:37:49.088+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.294 seconds
[2024-02-06T21:38:19.571+0100] {processor.py:161} INFO - Started process (PID=26892) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:38:19.631+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:38:19.671+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:38:19.649+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:38:19.807+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:38:19.798+0100] {dagbag.py:348} ERROR - Failed to import: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/airflow/dags/datalake_dag.py", line 7, in <module>
    from extraction import extraction_from_api_and_save_to_hdfs
  File "/home/ubuntu/airflow/extraction.py", line 3, in <module>
    from utils.api_utils import get_data_from_api_and_save_to_hdfs, get_data_from_api, \
  File "/home/ubuntu/airflow/utils/api_utils.py", line 2, in <module>
    from .hdfs_utils import send_file_to_hdfs
  File "/home/ubuntu/airflow/utils/hdfs_utils.py", line 1, in <module>
    from hdfs import InsecureClient
ModuleNotFoundError: No module named 'hdfs'
[2024-02-06T21:38:19.808+0100] {processor.py:842} WARNING - No viable dags retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:38:19.865+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.367 seconds
[2024-02-06T21:38:50.287+0100] {processor.py:161} INFO - Started process (PID=26986) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:38:50.293+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:38:50.297+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:38:50.295+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:38:50.401+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:38:50.795+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:38:50.795+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:38:50.855+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:38:50.855+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:38:50.960+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.681 seconds
[2024-02-06T21:39:21.304+0100] {processor.py:161} INFO - Started process (PID=27154) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:39:21.333+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:39:21.337+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:39:21.335+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:39:21.465+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:39:21.553+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:39:21.552+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:39:21.601+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:39:21.601+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:39:21.682+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.388 seconds
[2024-02-06T21:39:53.067+0100] {processor.py:161} INFO - Started process (PID=27384) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:39:53.071+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:39:53.073+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:39:53.072+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:39:53.107+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:39:53.122+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:39:53.122+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:39:53.215+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:39:53.215+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:39:53.352+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.300 seconds
[2024-02-06T21:40:25.120+0100] {processor.py:161} INFO - Started process (PID=27587) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:40:25.124+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:40:25.127+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:40:25.125+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:40:25.180+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:40:25.528+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:40:25.528+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:40:25.631+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:40:25.630+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:40:25.719+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.608 seconds
[2024-02-06T21:41:13.582+0100] {processor.py:161} INFO - Started process (PID=27949) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:41:13.586+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:41:13.589+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:41:13.588+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:41:13.622+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:41:13.663+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:41:13.662+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:41:13.708+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:41:13.707+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:41:13.753+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.179 seconds
[2024-02-06T21:42:34.685+0100] {processor.py:161} INFO - Started process (PID=28418) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:42:34.690+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:42:34.693+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:42:34.692+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:42:34.740+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:42:34.782+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:42:34.782+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:42:34.842+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:42:34.842+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:42:34.924+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.248 seconds
[2024-02-06T21:42:36.147+0100] {processor.py:161} INFO - Started process (PID=28420) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:42:36.149+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:42:36.152+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:42:36.151+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:42:36.183+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:42:36.218+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:42:36.218+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:42:36.271+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:42:36.271+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:42:36.396+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.255 seconds
[2024-02-06T21:43:45.976+0100] {processor.py:161} INFO - Started process (PID=28849) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:43:45.980+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:43:45.982+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:43:45.981+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:43:46.017+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:43:46.048+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:43:46.048+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:43:46.097+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:43:46.096+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:43:46.165+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.198 seconds
[2024-02-06T21:43:47.934+0100] {processor.py:161} INFO - Started process (PID=28870) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:43:47.938+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:43:47.941+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:43:47.940+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:43:47.984+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:43:48.056+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:43:48.055+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:43:48.137+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:43:48.137+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:43:48.213+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.286 seconds
[2024-02-06T21:45:03.285+0100] {processor.py:161} INFO - Started process (PID=29277) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:45:03.297+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:45:03.299+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:45:03.299+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:45:03.327+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:45:03.363+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:45:03.362+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:45:03.416+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:45:03.416+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:45:03.480+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.201 seconds
[2024-02-06T21:45:35.069+0100] {processor.py:161} INFO - Started process (PID=29388) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:45:35.075+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:45:35.081+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:45:35.077+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:45:35.124+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:45:35.176+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:45:35.166+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:45:35.179+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:45:35.178+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:45:35.311+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:45:35.310+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:45:35.430+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.375 seconds
[2024-02-06T21:46:06.111+0100] {processor.py:161} INFO - Started process (PID=29463) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:46:06.127+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:46:06.129+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:46:06.128+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:46:06.173+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:46:06.230+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:46:06.224+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:46:06.233+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:46:06.232+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:46:06.314+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:46:06.313+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:46:06.456+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.362 seconds
[2024-02-06T21:46:36.825+0100] {processor.py:161} INFO - Started process (PID=29581) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:46:36.829+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:46:36.833+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:46:36.832+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:46:36.860+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:46:36.886+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:46:36.884+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:46:36.888+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:46:36.888+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:46:36.926+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:46:36.926+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:46:37.002+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.183 seconds
[2024-02-06T21:47:07.474+0100] {processor.py:161} INFO - Started process (PID=29618) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:47:07.476+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:47:07.479+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:47:07.478+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:47:07.508+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:47:07.528+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:47:07.526+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:47:07.530+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:47:07.529+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:47:07.565+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:47:07.565+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:47:07.655+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.187 seconds
[2024-02-06T21:47:38.152+0100] {processor.py:161} INFO - Started process (PID=29673) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:47:38.159+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:47:38.161+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:47:38.160+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:47:38.189+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:47:38.206+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:47:38.204+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:47:38.207+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:47:38.207+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:47:38.237+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:47:38.237+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:47:38.313+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.167 seconds
[2024-02-06T21:48:08.820+0100] {processor.py:161} INFO - Started process (PID=29684) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:48:08.825+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:48:08.828+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:48:08.827+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:48:08.854+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:48:08.874+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:48:08.872+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:48:08.875+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:48:08.875+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:48:08.926+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:48:08.925+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:48:09.013+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.202 seconds
[2024-02-06T21:48:39.951+0100] {processor.py:161} INFO - Started process (PID=29695) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:48:39.955+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:48:39.958+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:48:39.957+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:48:39.983+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:48:40.009+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:48:40.006+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:48:40.012+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:48:40.012+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:48:40.041+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:48:40.040+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:48:40.125+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.181 seconds
[2024-02-06T21:49:10.186+0100] {processor.py:161} INFO - Started process (PID=29709) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:49:10.189+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:49:10.192+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:49:10.191+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:49:10.217+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:49:10.238+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:49:10.235+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:49:10.240+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:49:10.239+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:49:10.277+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:49:10.277+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:49:10.358+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.178 seconds
[2024-02-06T21:49:40.588+0100] {processor.py:161} INFO - Started process (PID=29736) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:49:40.598+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:49:40.601+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:49:40.600+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:49:40.663+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:49:40.704+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:49:40.699+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:49:40.707+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:49:40.706+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:49:40.762+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:49:40.762+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:49:40.851+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.291 seconds
[2024-02-06T21:50:12.158+0100] {processor.py:161} INFO - Started process (PID=29844) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:50:12.165+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:50:12.174+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:50:12.167+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:50:12.251+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:50:12.364+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:50:12.344+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:50:12.365+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:50:12.364+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:50:12.447+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:50:12.447+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:50:12.717+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.569 seconds
[2024-02-06T21:50:43.156+0100] {processor.py:161} INFO - Started process (PID=29890) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:50:43.159+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:50:43.161+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:50:43.161+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:50:43.198+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:50:43.224+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:50:43.220+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:50:43.225+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:50:43.224+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:50:43.265+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:50:43.264+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:50:43.332+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.240 seconds
[2024-02-06T21:51:13.626+0100] {processor.py:161} INFO - Started process (PID=30052) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:51:13.628+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:51:13.632+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:51:13.630+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:51:13.660+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:51:13.698+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:51:13.689+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:51:13.699+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:51:13.699+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:51:13.777+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:51:13.777+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:51:13.876+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.260 seconds
[2024-02-06T21:51:44.202+0100] {processor.py:161} INFO - Started process (PID=30151) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:51:44.205+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:51:44.208+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:51:44.207+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:51:44.251+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:51:44.275+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:51:44.273+0100] {dagbag.py:647} ERROR - Failed to write serialized DAG: /home/ubuntu/airflow/dags/datalake_dag.py
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 166, in write_dag
  File "<string>", line 4, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 481, in _initialize_instance
    with util.safe_reraise():
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/ubuntu/.local/lib/python3.10/site-packages/sqlalchemy/orm/state.py", line 479, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/serialized_dag.py", line 113, in __init__
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1434, in to_dict
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 353, in validate_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 70, in load_dag_schema
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/json_schema.py", line 57, in load_dag_schema_dict
  File "/usr/lib/python3.10/pkgutil.py", line 639, in get_data
    return loader.get_data(resource_name)
  File "<frozen importlib._bootstrap_external>", line 1073, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.10/site-packages/airflow/serialization/schema.json'
[2024-02-06T21:51:44.276+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:51:44.276+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:51:44.316+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:51:44.316+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:51:44.399+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.235 seconds
[2024-02-06T21:52:14.714+0100] {processor.py:161} INFO - Started process (PID=30291) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:52:14.718+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:52:14.721+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:52:14.720+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:52:14.769+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:52:14.814+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:52:14.812+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:52:14.866+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:52:14.864+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:52:15.052+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.352 seconds
[2024-02-06T21:52:45.548+0100] {processor.py:161} INFO - Started process (PID=30318) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:52:45.552+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:52:45.556+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:52:45.555+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:52:45.612+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:52:45.679+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:52:45.670+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:52:45.771+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:52:45.769+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:52:45.866+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.333 seconds
[2024-02-06T21:53:17.042+0100] {processor.py:161} INFO - Started process (PID=30489) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:53:17.083+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:53:17.115+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:53:17.096+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:53:18.115+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:53:18.263+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:53:18.262+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:53:18.511+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:53:18.509+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:53:19.142+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 2.149 seconds
[2024-02-06T21:53:50.114+0100] {processor.py:161} INFO - Started process (PID=30800) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:53:50.125+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:53:50.143+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:53:50.141+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:53:50.188+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:53:50.835+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:53:50.835+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:53:51.045+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:53:51.044+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:53:51.277+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 1.265 seconds
[2024-02-06T21:54:22.059+0100] {processor.py:161} INFO - Started process (PID=31250) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:22.063+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:54:22.067+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:22.065+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:22.096+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:22.122+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:22.121+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:54:22.149+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:22.149+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:54:22.203+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.152 seconds
[2024-02-06T21:54:52.487+0100] {processor.py:161} INFO - Started process (PID=31505) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:52.492+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:54:52.495+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:52.494+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:52.532+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:52.565+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:52.564+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:54:52.633+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:52.632+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:54:52.675+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.196 seconds
[2024-02-06T21:54:53.225+0100] {processor.py:161} INFO - Started process (PID=31506) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:53.227+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:54:53.230+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:53.229+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:53.261+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:54:53.286+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:53.286+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:54:53.313+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:54:53.313+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:54:53.375+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.155 seconds
[2024-02-06T21:55:22.907+0100] {processor.py:161} INFO - Started process (PID=31727) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:55:22.911+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:55:22.915+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:55:22.913+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:55:22.987+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:55:23.027+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:55:23.026+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:55:23.115+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:55:23.115+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:55:23.188+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.309 seconds
[2024-02-06T21:56:26.232+0100] {processor.py:161} INFO - Started process (PID=32358) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:56:26.236+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:56:26.243+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:56:26.240+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:56:26.279+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:56:26.318+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:56:26.318+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:56:26.362+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:56:26.361+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:56:26.469+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.243 seconds
[2024-02-06T21:57:36.449+0100] {processor.py:161} INFO - Started process (PID=32910) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:57:36.470+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:57:36.473+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:57:36.472+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:57:36.538+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:57:36.623+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:57:36.622+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:57:36.703+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:57:36.702+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:57:36.765+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.329 seconds
[2024-02-06T21:57:59.154+0100] {processor.py:161} INFO - Started process (PID=33117) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:57:59.171+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:57:59.173+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:57:59.172+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:57:59.240+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:57:59.297+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:57:59.297+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:57:59.455+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:57:59.454+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:57:59.637+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.495 seconds
[2024-02-06T21:59:24.585+0100] {processor.py:161} INFO - Started process (PID=34017) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:59:24.591+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T21:59:24.608+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:59:24.607+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:59:24.660+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T21:59:25.013+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:59:25.013+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T21:59:25.118+0100] {logging_mixin.py:188} INFO - [2024-02-06T21:59:25.118+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T21:59:25.262+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.695 seconds
[2024-02-06T22:00:09.494+0100] {processor.py:161} INFO - Started process (PID=34406) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:00:09.499+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:00:09.502+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:00:09.501+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:00:09.607+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:00:09.644+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:00:09.644+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:00:09.795+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:00:09.795+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:00:09.973+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.519 seconds
[2024-02-06T22:01:08.447+0100] {processor.py:161} INFO - Started process (PID=34842) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:01:08.489+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:01:08.492+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:01:08.491+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:01:08.543+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:01:08.681+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:01:08.681+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:01:08.792+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:01:08.792+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:01:08.993+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.555 seconds
[2024-02-06T22:01:48.279+0100] {processor.py:161} INFO - Started process (PID=35048) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:01:48.295+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:01:48.301+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:01:48.301+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:01:48.379+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:01:48.420+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:01:48.416+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:01:48.502+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:01:48.502+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:01:48.562+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.289 seconds
[2024-02-06T22:02:34.432+0100] {processor.py:161} INFO - Started process (PID=35541) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:02:34.448+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:02:34.461+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:02:34.457+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:02:34.519+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:02:34.593+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:02:34.592+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:02:34.661+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:02:34.655+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:02:34.821+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.404 seconds
[2024-02-06T22:03:25.037+0100] {processor.py:161} INFO - Started process (PID=36016) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:03:25.042+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:03:25.044+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:03:25.044+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:03:25.075+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:03:25.104+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:03:25.103+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:03:25.138+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:03:25.138+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:03:25.196+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.167 seconds
[2024-02-06T22:04:19.332+0100] {processor.py:161} INFO - Started process (PID=36655) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:04:19.342+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:04:19.346+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:04:19.345+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:04:19.701+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:04:19.788+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:04:19.776+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:04:19.881+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:04:19.881+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:04:20.009+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.710 seconds
[2024-02-06T22:07:10.721+0100] {processor.py:161} INFO - Started process (PID=37967) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:07:10.779+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:07:10.784+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:07:10.783+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:07:10.859+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:07:11.256+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:07:11.256+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:07:11.297+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:07:11.297+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:07:11.354+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.640 seconds
[2024-02-06T22:14:04.843+0100] {processor.py:161} INFO - Started process (PID=40625) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:14:04.852+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:14:04.856+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:14:04.855+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:14:04.894+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:14:05.244+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:14:05.243+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:14:05.293+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:14:05.293+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:14:05.356+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.518 seconds
[2024-02-06T22:15:57.293+0100] {processor.py:161} INFO - Started process (PID=41702) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:15:57.297+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T22:15:57.301+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:15:57.300+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:15:57.357+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T22:15:57.402+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:15:57.402+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T22:15:57.455+0100] {logging_mixin.py:188} INFO - [2024-02-06T22:15:57.455+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-01 00:00:00+00:00, run_after=2024-01-02 00:00:00+00:00
[2024-02-06T22:15:57.542+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 0.256 seconds
[2024-02-06T23:15:41.084+0100] {processor.py:161} INFO - Started process (PID=65020) to work on /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T23:15:41.116+0100] {processor.py:830} INFO - Processing file /home/ubuntu/airflow/dags/datalake_dag.py for tasks to queue
[2024-02-06T23:15:41.134+0100] {logging_mixin.py:188} INFO - [2024-02-06T23:15:41.125+0100] {dagbag.py:538} INFO - Filling up the DagBag from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T23:15:42.060+0100] {logging_mixin.py:188} INFO - Data fetch successfully
[2024-02-06T23:15:42.073+0100] {logging_mixin.py:188} INFO - [2024-02-06T23:15:42.071+0100] {client.py:192} INFO - Instantiated <InsecureClient(url='http://localhost:9870')>.
[2024-02-06T23:15:42.078+0100] {logging_mixin.py:188} INFO - [2024-02-06T23:15:42.077+0100] {client.py:496} INFO - Writing to '/datalake/raw/match/2024-02-06.json'.
[2024-02-06T23:15:43.137+0100] {logging_mixin.py:188} INFO - téléversement vers HDFS réussi
[2024-02-06T23:15:43.184+0100] {processor.py:840} INFO - DAG(s) 'Competition_dag' retrieved from /home/ubuntu/airflow/dags/datalake_dag.py
[2024-02-06T23:15:43.654+0100] {logging_mixin.py:188} INFO - [2024-02-06T23:15:43.651+0100] {dag.py:3036} INFO - Sync 1 DAGs
[2024-02-06T23:15:43.736+0100] {logging_mixin.py:188} INFO - [2024-02-06T23:15:43.736+0100] {dag.py:3823} INFO - Setting next_dagrun for Competition_dag to 2024-01-05 00:00:00+00:00, run_after=2024-01-06 00:00:00+00:00
[2024-02-06T23:15:43.892+0100] {processor.py:183} INFO - Processing /home/ubuntu/airflow/dags/datalake_dag.py took 2.897 seconds
