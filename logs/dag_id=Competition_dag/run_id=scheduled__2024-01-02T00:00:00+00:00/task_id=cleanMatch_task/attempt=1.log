[2024-02-06T22:18:38.450+0100] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-02-06T22:18:38.571+0100] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-02-06T22:18:38.573+0100] {taskinstance.py:2170} INFO - Starting attempt 1 of 6
[2024-02-06T22:18:38.666+0100] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): cleanMatch_task> on 2024-01-02 00:00:00+00:00
[2024-02-06T22:18:38.701+0100] {standard_task_runner.py:60} INFO - Started process 43558 to run task
[2024-02-06T22:18:38.879+0100] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'Competition_dag', 'cleanMatch_task', 'scheduled__2024-01-02T00:00:00+00:00', '--job-id', '31', '--raw', '--subdir', 'DAGS_FOLDER/datalake_dag.py', '--cfg-path', '/tmp/tmp4kqgs0ts']
[2024-02-06T22:18:38.956+0100] {standard_task_runner.py:88} INFO - Job 31: Subtask cleanMatch_task
[2024-02-06T22:18:39.139+0100] {task_command.py:423} INFO - Running <TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [running]> on host ubuntu.ubuntu.virtualbox.org
[2024-02-06T22:18:39.481+0100] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='NOUMEN_Frech_Warren' AIRFLOW_CTX_DAG_ID='Competition_dag' AIRFLOW_CTX_TASK_ID='cleanMatch_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-02T00:00:00+00:00'
[2024-02-06T22:18:39.518+0100] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-02-06T22:18:39.548+0100] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master yarn --conf spark.airflow.execution_date=2024-01-02 --name arrow-spark --class FormattedData --queue root.default /home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar
[2024-02-06T22:18:52.285+0100] {spark_submit.py:571} INFO - 24/02/06 22:18:52 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
[2024-02-06T22:18:52.347+0100] {spark_submit.py:571} INFO - 24/02/06 22:18:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-02-06T22:18:52.604+0100] {spark_submit.py:571} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2024-02-06T22:18:52.605+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:634)
[2024-02-06T22:18:52.606+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:274)
[2024-02-06T22:18:52.606+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:234)
[2024-02-06T22:18:52.607+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:119)
[2024-02-06T22:18:52.607+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1055)
[2024-02-06T22:18:52.607+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1055)
[2024-02-06T22:18:52.608+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)
[2024-02-06T22:18:52.608+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1072)
[2024-02-06T22:18:52.609+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1081)
[2024-02-06T22:18:52.609+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2024-02-06T22:18:52.875+0100] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --conf spark.airflow.execution_date=2024-01-02 --name arrow-spark --class FormattedData --queue root.default /home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar. Error code is: 1.
[2024-02-06T22:18:52.896+0100] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=Competition_dag, task_id=cleanMatch_task, execution_date=20240102T000000, start_date=20240206T211838, end_date=20240206T211852
[2024-02-06T22:18:53.094+0100] {standard_task_runner.py:107} ERROR - Failed to execute job 31 for task cleanMatch_task (Cannot execute: spark-submit --master yarn --conf spark.airflow.execution_date=2024-01-02 --name arrow-spark --class FormattedData --queue root.default /home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar. Error code is: 1.; 43558)
[2024-02-06T22:18:53.135+0100] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-06T22:18:53.259+0100] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-06T22:31:59.778+0100] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-02-06T22:31:59.809+0100] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-02-06T22:31:59.811+0100] {taskinstance.py:2170} INFO - Starting attempt 1 of 6
[2024-02-06T22:31:59.893+0100] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): cleanMatch_task> on 2024-01-02 00:00:00+00:00
[2024-02-06T22:31:59.912+0100] {standard_task_runner.py:60} INFO - Started process 53104 to run task
[2024-02-06T22:31:59.930+0100] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'Competition_dag', 'cleanMatch_task', 'scheduled__2024-01-02T00:00:00+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/datalake_dag.py', '--cfg-path', '/tmp/tmp5206a6qb']
[2024-02-06T22:31:59.934+0100] {standard_task_runner.py:88} INFO - Job 33: Subtask cleanMatch_task
[2024-02-06T22:32:00.148+0100] {task_command.py:423} INFO - Running <TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [running]> on host ubuntu.ubuntu.virtualbox.org
[2024-02-06T22:32:00.790+0100] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='NOUMEN_Frech_Warren' AIRFLOW_CTX_DAG_ID='Competition_dag' AIRFLOW_CTX_TASK_ID='cleanMatch_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-02T00:00:00+00:00'
[2024-02-06T22:32:00.815+0100] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-02-06T22:32:00.836+0100] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master local[*] --conf spark.airflow.execution_date=2024-01-02 --name arrow-spark --class FormattedData --queue root.default --deploy-mode client /home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar
[2024-02-06T22:32:14.142+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:14 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
[2024-02-06T22:32:14.163+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-02-06T22:32:17.243+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:17 INFO SparkContext: Running Spark version 3.3.4
[2024-02-06T22:32:17.566+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-02-06T22:32:18.352+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO ResourceUtils: ==============================================================
[2024-02-06T22:32:18.354+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-02-06T22:32:18.354+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO ResourceUtils: ==============================================================
[2024-02-06T22:32:18.492+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO SparkContext: Submitted application: JobFormatted
[2024-02-06T22:32:18.493+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-02-06T22:32:18.667+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO ResourceProfile: Limiting resource is cpu
[2024-02-06T22:32:18.679+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-02-06T22:32:18.855+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO SecurityManager: Changing view acls to: ubuntu
[2024-02-06T22:32:18.901+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO SecurityManager: Changing modify acls to: ubuntu
[2024-02-06T22:32:18.905+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO SecurityManager: Changing view acls groups to:
[2024-02-06T22:32:18.913+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO SecurityManager: Changing modify acls groups to:
[2024-02-06T22:32:18.914+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()
[2024-02-06T22:32:21.414+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:21 INFO Utils: Successfully started service 'sparkDriver' on port 44229.
[2024-02-06T22:32:21.827+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:21 INFO SparkEnv: Registering MapOutputTracker
[2024-02-06T22:32:22.092+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:22 INFO SparkEnv: Registering BlockManagerMaster
[2024-02-06T22:32:22.188+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-02-06T22:32:22.195+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-02-06T22:32:22.236+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-02-06T22:32:22.293+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9a6a2f72-071f-4e6e-9a85-6ceffc54c63a
[2024-02-06T22:32:22.607+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-02-06T22:32:22.725+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-02-06T22:32:24.313+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-02-06T22:32:24.770+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:24 INFO SparkContext: Added JAR file:/home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar at spark://10.0.2.15:44229/jars/scala_2.13-0.1.0.jar with timestamp 1707255137151
[2024-02-06T22:32:25.502+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:25 INFO Executor: Starting executor ID driver on host 10.0.2.15
[2024-02-06T22:32:25.527+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-02-06T22:32:25.817+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:25 INFO Executor: Fetching spark://10.0.2.15:44229/jars/scala_2.13-0.1.0.jar with timestamp 1707255137151
[2024-02-06T22:32:26.521+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:26 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:44229 after 626 ms (0 ms spent in bootstraps)
[2024-02-06T22:32:26.828+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:26 INFO Utils: Fetching spark://10.0.2.15:44229/jars/scala_2.13-0.1.0.jar to /tmp/spark-ed1e3776-a77d-4340-8587-629ef16b673e/userFiles-f01900dc-4067-461e-a6af-c9c627809cf0/fetchFileTemp5759248554197189021.tmp
[2024-02-06T22:32:27.568+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:27 INFO Executor: Adding file:/tmp/spark-ed1e3776-a77d-4340-8587-629ef16b673e/userFiles-f01900dc-4067-461e-a6af-c9c627809cf0/scala_2.13-0.1.0.jar to class loader
[2024-02-06T22:32:27.744+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34949.
[2024-02-06T22:32:27.747+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:27 INFO NettyBlockTransferService: Server created on 10.0.2.15:34949
[2024-02-06T22:32:27.758+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-02-06T22:32:27.817+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 34949, None)
[2024-02-06T22:32:27.893+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:34949 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 34949, None)
[2024-02-06T22:32:28.008+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 34949, None)
[2024-02-06T22:32:28.026+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 34949, None)
[2024-02-06T22:32:31.771+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-02-06T22:32:31.848+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:31 INFO SharedState: Warehouse path is 'file:/home/ubuntu/airflow/spark-warehouse'.
[2024-02-06T22:32:36.686+0100] {spark_submit.py:571} INFO - Exception in thread "main" java.lang.NoClassDefFoundError: scala/collection/StringOps$
[2024-02-06T22:32:36.690+0100] {spark_submit.py:571} INFO - at FormattedData$.main(FormattedData.scala:14)
[2024-02-06T22:32:36.696+0100] {spark_submit.py:571} INFO - at FormattedData.main(FormattedData.scala)
[2024-02-06T22:32:36.698+0100] {spark_submit.py:571} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2024-02-06T22:32:36.699+0100] {spark_submit.py:571} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2024-02-06T22:32:36.700+0100] {spark_submit.py:571} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2024-02-06T22:32:36.703+0100] {spark_submit.py:571} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2024-02-06T22:32:36.705+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2024-02-06T22:32:36.707+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:984)
[2024-02-06T22:32:36.708+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:191)
[2024-02-06T22:32:36.709+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:214)
[2024-02-06T22:32:36.710+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2024-02-06T22:32:36.712+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1072)
[2024-02-06T22:32:36.713+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1081)
[2024-02-06T22:32:36.714+0100] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2024-02-06T22:32:36.715+0100] {spark_submit.py:571} INFO - Caused by: java.lang.ClassNotFoundException: scala.collection.StringOps$
[2024-02-06T22:32:36.716+0100] {spark_submit.py:571} INFO - at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
[2024-02-06T22:32:36.717+0100] {spark_submit.py:571} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
[2024-02-06T22:32:36.719+0100] {spark_submit.py:571} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
[2024-02-06T22:32:36.720+0100] {spark_submit.py:571} INFO - ... 14 more
[2024-02-06T22:32:36.989+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:36 INFO SparkContext: Invoking stop() from shutdown hook
[2024-02-06T22:32:37.299+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:37 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
[2024-02-06T22:32:37.917+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-02-06T22:32:38.216+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:38 INFO MemoryStore: MemoryStore cleared
[2024-02-06T22:32:38.259+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:38 INFO BlockManager: BlockManager stopped
[2024-02-06T22:32:38.294+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:38 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-02-06T22:32:38.338+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-02-06T22:32:38.674+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:38 INFO SparkContext: Successfully stopped SparkContext
[2024-02-06T22:32:38.676+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:38 INFO ShutdownHookManager: Shutdown hook called
[2024-02-06T22:32:38.799+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed1e3776-a77d-4340-8587-629ef16b673e
[2024-02-06T22:32:38.944+0100] {spark_submit.py:571} INFO - 24/02/06 22:32:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-7364c47c-94d3-4362-98f0-6c7e7be47111
[2024-02-06T22:32:39.640+0100] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local[*] --conf spark.airflow.execution_date=2024-01-02 --name arrow-spark --class FormattedData --queue root.default --deploy-mode client /home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar. Error code is: 1.
[2024-02-06T22:32:39.689+0100] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=Competition_dag, task_id=cleanMatch_task, execution_date=20240102T000000, start_date=20240206T213159, end_date=20240206T213239
[2024-02-06T22:32:39.768+0100] {standard_task_runner.py:107} ERROR - Failed to execute job 33 for task cleanMatch_task (Cannot execute: spark-submit --master local[*] --conf spark.airflow.execution_date=2024-01-02 --name arrow-spark --class FormattedData --queue root.default --deploy-mode client /home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar. Error code is: 1.; 53104)
[2024-02-06T22:32:39.857+0100] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-06T22:32:39.916+0100] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-06T23:45:41.009+0100] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-02-06T23:45:41.198+0100] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [queued]>
[2024-02-06T23:45:41.200+0100] {taskinstance.py:2170} INFO - Starting attempt 1 of 6
[2024-02-06T23:45:41.319+0100] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): cleanMatch_task> on 2024-01-02 00:00:00+00:00
[2024-02-06T23:45:41.370+0100] {standard_task_runner.py:60} INFO - Started process 82614 to run task
[2024-02-06T23:45:41.435+0100] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'Competition_dag', 'cleanMatch_task', 'scheduled__2024-01-02T00:00:00+00:00', '--job-id', '47', '--raw', '--subdir', 'DAGS_FOLDER/datalake_dag.py', '--cfg-path', '/tmp/tmpg0mxfxfl']
[2024-02-06T23:45:41.447+0100] {standard_task_runner.py:88} INFO - Job 47: Subtask cleanMatch_task
[2024-02-06T23:45:41.848+0100] {task_command.py:423} INFO - Running <TaskInstance: Competition_dag.cleanMatch_task scheduled__2024-01-02T00:00:00+00:00 [running]> on host ubuntu.ubuntu.virtualbox.org
[2024-02-06T23:45:42.336+0100] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='NOUMEN_Frech_Warren' AIRFLOW_CTX_DAG_ID='Competition_dag' AIRFLOW_CTX_TASK_ID='cleanMatch_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-02T00:00:00+00:00'
[2024-02-06T23:45:42.444+0100] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-02-06T23:45:42.448+0100] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master local[*] --conf spark.airflow.execution_date=2024-01-02 --name arrow-spark --class FormattedData --queue root.default --deploy-mode client /home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar
[2024-02-06T23:46:11.680+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:11 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
[2024-02-06T23:46:11.710+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-02-06T23:46:18.562+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:18 INFO SparkContext: Running Spark version 3.3.4
[2024-02-06T23:46:19.312+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-02-06T23:46:21.416+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:21 INFO ResourceUtils: ==============================================================
[2024-02-06T23:46:21.426+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-02-06T23:46:21.429+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:21 INFO ResourceUtils: ==============================================================
[2024-02-06T23:46:21.437+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:21 INFO SparkContext: Submitted application: JobFormatted
[2024-02-06T23:46:21.886+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-02-06T23:46:21.932+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:21 INFO ResourceProfile: Limiting resource is cpu
[2024-02-06T23:46:21.935+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-02-06T23:46:22.698+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:22 INFO SecurityManager: Changing view acls to: ubuntu
[2024-02-06T23:46:22.712+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:22 INFO SecurityManager: Changing modify acls to: ubuntu
[2024-02-06T23:46:22.734+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:22 INFO SecurityManager: Changing view acls groups to:
[2024-02-06T23:46:22.738+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:22 INFO SecurityManager: Changing modify acls groups to:
[2024-02-06T23:46:22.751+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()
[2024-02-06T23:46:26.773+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:26 INFO Utils: Successfully started service 'sparkDriver' on port 45799.
[2024-02-06T23:46:27.428+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:27 INFO SparkEnv: Registering MapOutputTracker
[2024-02-06T23:46:27.686+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:27 INFO SparkEnv: Registering BlockManagerMaster
[2024-02-06T23:46:27.855+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-02-06T23:46:27.857+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-02-06T23:46:28.020+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-02-06T23:46:28.799+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5e30d55c-eccd-4cc0-bc45-50e436aee4f3
[2024-02-06T23:46:29.032+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-02-06T23:46:29.357+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-02-06T23:46:32.385+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-02-06T23:46:32.883+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:32 INFO SparkContext: Added JAR file:/home/ubuntu/airflow/scala/target/scala-2.13/scala_2.13-0.1.0.jar at spark://10.0.2.15:45799/jars/scala_2.13-0.1.0.jar with timestamp 1707259578309
[2024-02-06T23:46:34.756+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:34 INFO Executor: Starting executor ID driver on host 10.0.2.15
[2024-02-06T23:46:34.816+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-02-06T23:46:35.039+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:35 INFO Executor: Fetching spark://10.0.2.15:45799/jars/scala_2.13-0.1.0.jar with timestamp 1707259578309
[2024-02-06T23:46:36.076+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:35 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:45799 after 785 ms (0 ms spent in bootstraps)
[2024-02-06T23:46:36.246+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:36 INFO Utils: Fetching spark://10.0.2.15:45799/jars/scala_2.13-0.1.0.jar to /tmp/spark-5c18d90c-71e3-42e2-b6e6-3d557ec5d5c9/userFiles-39f6f9ef-81a5-4120-9d9c-ad4038a4aab5/fetchFileTemp8546774556293404869.tmp
[2024-02-06T23:46:37.139+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:37 INFO Executor: Adding file:/tmp/spark-5c18d90c-71e3-42e2-b6e6-3d557ec5d5c9/userFiles-39f6f9ef-81a5-4120-9d9c-ad4038a4aab5/scala_2.13-0.1.0.jar to class loader
[2024-02-06T23:46:37.339+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38351.
[2024-02-06T23:46:37.344+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:37 INFO NettyBlockTransferService: Server created on 10.0.2.15:38351
[2024-02-06T23:46:37.395+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-02-06T23:46:37.483+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 38351, None)
[2024-02-06T23:46:37.661+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:38351 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 38351, None)
[2024-02-06T23:46:37.701+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 38351, None)
[2024-02-06T23:46:37.706+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 38351, None)
[2024-02-06T23:46:41.090+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:41 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener AppStatusListener took 1.01257158s.
[2024-02-06T23:46:44.895+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-02-06T23:46:44.994+0100] {spark_submit.py:571} INFO - 24/02/06 23:46:44 INFO SharedState: Warehouse path is 'file:/home/ubuntu/airflow/scala/spark-warehouse'.
[2024-02-06T23:47:07.169+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:07 INFO InMemoryFileIndex: It took 1613 ms to list leaf files for 1 paths.
[2024-02-06T23:47:08.585+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:08 INFO InMemoryFileIndex: It took 134 ms to list leaf files for 1 paths.
[2024-02-06T23:47:29.133+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:29 INFO FileSourceStrategy: Pushed Filters:
[2024-02-06T23:47:29.204+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:29 INFO FileSourceStrategy: Post-Scan Filters:
[2024-02-06T23:47:29.205+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:29 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-02-06T23:47:32.582+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 206.0 KiB, free 434.2 MiB)
[2024-02-06T23:47:33.706+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 434.2 MiB)
[2024-02-06T23:47:33.760+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:38351 (size: 34.2 KiB, free: 434.4 MiB)
[2024-02-06T23:47:33.864+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:33 INFO SparkContext: Created broadcast 0 from json at FormattedData.scala:21
[2024-02-06T23:47:33.954+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-02-06T23:47:35.640+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:35 INFO SparkContext: Starting job: json at FormattedData.scala:21
[2024-02-06T23:47:35.813+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:35 INFO DAGScheduler: Got job 0 (json at FormattedData.scala:21) with 1 output partitions
[2024-02-06T23:47:35.818+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:35 INFO DAGScheduler: Final stage: ResultStage 0 (json at FormattedData.scala:21)
[2024-02-06T23:47:35.877+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:35 INFO DAGScheduler: Parents of final stage: List()
[2024-02-06T23:47:35.879+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:35 INFO DAGScheduler: Missing parents: List()
[2024-02-06T23:47:35.907+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at FormattedData.scala:21), which has no missing parents
[2024-02-06T23:47:37.112+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.7 KiB, free 434.2 MiB)
[2024-02-06T23:47:37.128+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.1 MiB)
[2024-02-06T23:47:37.134+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:38351 (size: 7.1 KiB, free: 434.4 MiB)
[2024-02-06T23:47:37.171+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
[2024-02-06T23:47:37.309+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at FormattedData.scala:21) (first 15 tasks are for partitions Vector(0))
[2024-02-06T23:47:37.310+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-02-06T23:47:38.069+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, ANY, 4927 bytes) taskResourceAssignments Map()
[2024-02-06T23:47:38.325+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-02-06T23:47:39.492+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:39 INFO FileScanRDD: Reading File path: hdfs://localhost:9000/datalake/raw/match/2024-01-02.json, range: 0-23381, partition values: [empty row]
[2024-02-06T23:47:40.803+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:40 INFO CodeGenerator: Code generated in 1076.997428 ms
[2024-02-06T23:47:42.620+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3086 bytes result sent to driver
[2024-02-06T23:47:42.768+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4797 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-06T23:47:42.777+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-02-06T23:47:42.992+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:42 INFO DAGScheduler: ResultStage 0 (json at FormattedData.scala:21) finished in 6.729 s
[2024-02-06T23:47:43.105+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-06T23:47:43.107+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-02-06T23:47:43.128+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:43 INFO DAGScheduler: Job 0 finished: json at FormattedData.scala:21, took 7.477895 s
[2024-02-06T23:47:44.966+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:44 INFO FileSourceStrategy: Pushed Filters:
[2024-02-06T23:47:44.989+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:44 INFO FileSourceStrategy: Post-Scan Filters:
[2024-02-06T23:47:45.004+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:44 INFO FileSourceStrategy: Output Data Schema: struct<area: struct<code: string, flag: string, id: bigint, name: string ... 2 more fields>, code: string, currentSeason: struct<currentMatchday: bigint, endDate: string, id: bigint, startDate: string, winner: string ... 3 more fields>, emblem: string, id: bigint ... 7 more fields>
[2024-02-06T23:47:46.027+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-06T23:47:46.398+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-06T23:47:46.402+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-06T23:47:46.464+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-06T23:47:46.524+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-06T23:47:46.524+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-06T23:47:46.545+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-06T23:47:47.317+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 205.9 KiB, free 433.9 MiB)
[2024-02-06T23:47:47.544+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 433.9 MiB)
[2024-02-06T23:47:47.615+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:38351 (size: 34.1 KiB, free: 434.3 MiB)
[2024-02-06T23:47:47.626+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:47 INFO SparkContext: Created broadcast 2 from parquet at FormattedData.scala:23
[2024-02-06T23:47:47.763+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-02-06T23:47:48.679+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO SparkContext: Starting job: parquet at FormattedData.scala:23
[2024-02-06T23:47:48.682+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO DAGScheduler: Got job 1 (parquet at FormattedData.scala:23) with 1 output partitions
[2024-02-06T23:47:48.688+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at FormattedData.scala:23)
[2024-02-06T23:47:48.689+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO DAGScheduler: Parents of final stage: List()
[2024-02-06T23:47:48.689+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO DAGScheduler: Missing parents: List()
[2024-02-06T23:47:48.747+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at FormattedData.scala:23), which has no missing parents
[2024-02-06T23:47:48.839+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:38351 in memory (size: 7.1 KiB, free: 434.3 MiB)
[2024-02-06T23:47:48.913+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:38351 in memory (size: 34.2 KiB, free: 434.4 MiB)
[2024-02-06T23:47:48.929+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 213.8 KiB, free 434.0 MiB)
[2024-02-06T23:47:49.050+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.8 KiB, free 433.9 MiB)
[2024-02-06T23:47:49.056+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:38351 (size: 76.8 KiB, free: 434.3 MiB)
[2024-02-06T23:47:49.057+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
[2024-02-06T23:47:49.063+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at FormattedData.scala:23) (first 15 tasks are for partitions Vector(0))
[2024-02-06T23:47:49.069+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-02-06T23:47:49.073+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, ANY, 4927 bytes) taskResourceAssignments Map()
[2024-02-06T23:47:49.085+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-02-06T23:47:49.539+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-06T23:47:49.541+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-06T23:47:49.543+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-06T23:47:49.544+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-06T23:47:49.544+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-06T23:47:49.553+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-06T23:47:49.576+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO CodecConfig: Compression: SNAPPY
[2024-02-06T23:47:49.583+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO CodecConfig: Compression: SNAPPY
[2024-02-06T23:47:49.961+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
[2024-02-06T23:47:49.965+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO ParquetOutputFormat: Validation is off
[2024-02-06T23:47:49.967+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2024-02-06T23:47:49.969+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:49 INFO ParquetOutputFormat: Parquet properties are:
[2024-02-06T23:47:49.969+0100] {spark_submit.py:571} INFO - Parquet page size to 1048576
[2024-02-06T23:47:49.969+0100] {spark_submit.py:571} INFO - Parquet dictionary page size to 1048576
[2024-02-06T23:47:49.970+0100] {spark_submit.py:571} INFO - Dictionary is true
[2024-02-06T23:47:49.970+0100] {spark_submit.py:571} INFO - Writer version is: PARQUET_1_0
[2024-02-06T23:47:49.970+0100] {spark_submit.py:571} INFO - Page size checking is: estimated
[2024-02-06T23:47:49.971+0100] {spark_submit.py:571} INFO - Min row count for page size check is: 100
[2024-02-06T23:47:49.971+0100] {spark_submit.py:571} INFO - Max row count for page size check is: 10000
[2024-02-06T23:47:49.971+0100] {spark_submit.py:571} INFO - Truncate length for column indexes is: 64
[2024-02-06T23:47:49.972+0100] {spark_submit.py:571} INFO - Truncate length for statistics min/max  is: 2147483647
[2024-02-06T23:47:49.973+0100] {spark_submit.py:571} INFO - Bloom filter enabled: false
[2024-02-06T23:47:49.974+0100] {spark_submit.py:571} INFO - Max Bloom filter size for a column is 1048576
[2024-02-06T23:47:49.974+0100] {spark_submit.py:571} INFO - Bloom filter expected number of distinct values are: null
[2024-02-06T23:47:49.992+0100] {spark_submit.py:571} INFO - Page row count limit to 20000
[2024-02-06T23:47:49.993+0100] {spark_submit.py:571} INFO - Writing page checksums is: on
[2024-02-06T23:47:50.414+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-02-06T23:47:50.416+0100] {spark_submit.py:571} INFO - {
[2024-02-06T23:47:50.418+0100] {spark_submit.py:571} INFO - "type" : "struct",
[2024-02-06T23:47:50.419+0100] {spark_submit.py:571} INFO - "fields" : [ {
[2024-02-06T23:47:50.419+0100] {spark_submit.py:571} INFO - "name" : "area",
[2024-02-06T23:47:50.420+0100] {spark_submit.py:571} INFO - "type" : {
[2024-02-06T23:47:50.420+0100] {spark_submit.py:571} INFO - "type" : "struct",
[2024-02-06T23:47:50.421+0100] {spark_submit.py:571} INFO - "fields" : [ {
[2024-02-06T23:47:50.421+0100] {spark_submit.py:571} INFO - "name" : "code",
[2024-02-06T23:47:50.422+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.422+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.423+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.423+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.424+0100] {spark_submit.py:571} INFO - "name" : "flag",
[2024-02-06T23:47:50.424+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.425+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.425+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.425+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.426+0100] {spark_submit.py:571} INFO - "name" : "id",
[2024-02-06T23:47:50.426+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-06T23:47:50.427+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.427+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.427+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.428+0100] {spark_submit.py:571} INFO - "name" : "name",
[2024-02-06T23:47:50.428+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.428+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.429+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.429+0100] {spark_submit.py:571} INFO - } ]
[2024-02-06T23:47:50.429+0100] {spark_submit.py:571} INFO - },
[2024-02-06T23:47:50.430+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.430+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.431+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.433+0100] {spark_submit.py:571} INFO - "name" : "code",
[2024-02-06T23:47:50.434+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.435+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.436+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.436+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.437+0100] {spark_submit.py:571} INFO - "name" : "currentSeason",
[2024-02-06T23:47:50.496+0100] {spark_submit.py:571} INFO - "type" : {
[2024-02-06T23:47:50.497+0100] {spark_submit.py:571} INFO - "type" : "struct",
[2024-02-06T23:47:50.498+0100] {spark_submit.py:571} INFO - "fields" : [ {
[2024-02-06T23:47:50.498+0100] {spark_submit.py:571} INFO - "name" : "currentMatchday",
[2024-02-06T23:47:50.499+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-06T23:47:50.541+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.543+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.544+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.544+0100] {spark_submit.py:571} INFO - "name" : "endDate",
[2024-02-06T23:47:50.544+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.545+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.545+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.545+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.545+0100] {spark_submit.py:571} INFO - "name" : "id",
[2024-02-06T23:47:50.545+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-06T23:47:50.546+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.546+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.546+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.546+0100] {spark_submit.py:571} INFO - "name" : "startDate",
[2024-02-06T23:47:50.546+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.547+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.547+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.547+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.547+0100] {spark_submit.py:571} INFO - "name" : "winner",
[2024-02-06T23:47:50.547+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.548+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.548+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.548+0100] {spark_submit.py:571} INFO - } ]
[2024-02-06T23:47:50.548+0100] {spark_submit.py:571} INFO - },
[2024-02-06T23:47:50.548+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.549+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.549+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.549+0100] {spark_submit.py:571} INFO - "name" : "emblem",
[2024-02-06T23:47:50.549+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.550+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.550+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.551+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.551+0100] {spark_submit.py:571} INFO - "name" : "id",
[2024-02-06T23:47:50.552+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-06T23:47:50.552+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.553+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.553+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.553+0100] {spark_submit.py:571} INFO - "name" : "lastUpdated",
[2024-02-06T23:47:50.553+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.553+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.554+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.554+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.554+0100] {spark_submit.py:571} INFO - "name" : "name",
[2024-02-06T23:47:50.554+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.554+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.554+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.555+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.555+0100] {spark_submit.py:571} INFO - "name" : "seasons",
[2024-02-06T23:47:50.555+0100] {spark_submit.py:571} INFO - "type" : {
[2024-02-06T23:47:50.555+0100] {spark_submit.py:571} INFO - "type" : "array",
[2024-02-06T23:47:50.555+0100] {spark_submit.py:571} INFO - "elementType" : {
[2024-02-06T23:47:50.556+0100] {spark_submit.py:571} INFO - "type" : "struct",
[2024-02-06T23:47:50.556+0100] {spark_submit.py:571} INFO - "fields" : [ {
[2024-02-06T23:47:50.556+0100] {spark_submit.py:571} INFO - "name" : "currentMatchday",
[2024-02-06T23:47:50.556+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-06T23:47:50.556+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.557+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.557+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.557+0100] {spark_submit.py:571} INFO - "name" : "endDate",
[2024-02-06T23:47:50.558+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.559+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.566+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.567+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.568+0100] {spark_submit.py:571} INFO - "name" : "id",
[2024-02-06T23:47:50.569+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-06T23:47:50.569+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.570+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.583+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.591+0100] {spark_submit.py:571} INFO - "name" : "startDate",
[2024-02-06T23:47:50.592+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.592+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.593+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.593+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.594+0100] {spark_submit.py:571} INFO - "name" : "winner",
[2024-02-06T23:47:50.594+0100] {spark_submit.py:571} INFO - "type" : {
[2024-02-06T23:47:50.598+0100] {spark_submit.py:571} INFO - "type" : "struct",
[2024-02-06T23:47:50.604+0100] {spark_submit.py:571} INFO - "fields" : [ {
[2024-02-06T23:47:50.605+0100] {spark_submit.py:571} INFO - "name" : "address",
[2024-02-06T23:47:50.605+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.606+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.614+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.614+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.615+0100] {spark_submit.py:571} INFO - "name" : "clubColors",
[2024-02-06T23:47:50.615+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.616+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.616+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.616+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.617+0100] {spark_submit.py:571} INFO - "name" : "crest",
[2024-02-06T23:47:50.617+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.617+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.617+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.618+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.618+0100] {spark_submit.py:571} INFO - "name" : "founded",
[2024-02-06T23:47:50.618+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-06T23:47:50.619+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.619+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.619+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.619+0100] {spark_submit.py:571} INFO - "name" : "id",
[2024-02-06T23:47:50.620+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-06T23:47:50.620+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.620+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.621+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.622+0100] {spark_submit.py:571} INFO - "name" : "lastUpdated",
[2024-02-06T23:47:50.622+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.622+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.623+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.623+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.623+0100] {spark_submit.py:571} INFO - "name" : "name",
[2024-02-06T23:47:50.624+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.624+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.625+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.625+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.626+0100] {spark_submit.py:571} INFO - "name" : "shortName",
[2024-02-06T23:47:50.626+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.626+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.627+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.627+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.627+0100] {spark_submit.py:571} INFO - "name" : "tla",
[2024-02-06T23:47:50.628+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.629+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.630+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.631+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.631+0100] {spark_submit.py:571} INFO - "name" : "venue",
[2024-02-06T23:47:50.631+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.632+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.632+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.639+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.640+0100] {spark_submit.py:571} INFO - "name" : "website",
[2024-02-06T23:47:50.641+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.642+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.643+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.643+0100] {spark_submit.py:571} INFO - } ]
[2024-02-06T23:47:50.644+0100] {spark_submit.py:571} INFO - },
[2024-02-06T23:47:50.644+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.649+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.650+0100] {spark_submit.py:571} INFO - } ]
[2024-02-06T23:47:50.650+0100] {spark_submit.py:571} INFO - },
[2024-02-06T23:47:50.650+0100] {spark_submit.py:571} INFO - "containsNull" : true
[2024-02-06T23:47:50.651+0100] {spark_submit.py:571} INFO - },
[2024-02-06T23:47:50.651+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.651+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.652+0100] {spark_submit.py:571} INFO - }, {
[2024-02-06T23:47:50.652+0100] {spark_submit.py:571} INFO - "name" : "type",
[2024-02-06T23:47:50.652+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-06T23:47:50.652+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-06T23:47:50.653+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-06T23:47:50.653+0100] {spark_submit.py:571} INFO - } ]
[2024-02-06T23:47:50.653+0100] {spark_submit.py:571} INFO - }
[2024-02-06T23:47:50.653+0100] {spark_submit.py:571} INFO - and corresponding Parquet message type:
[2024-02-06T23:47:50.653+0100] {spark_submit.py:571} INFO - message spark_schema {
[2024-02-06T23:47:50.654+0100] {spark_submit.py:571} INFO - optional group area {
[2024-02-06T23:47:50.654+0100] {spark_submit.py:571} INFO - optional binary code (STRING);
[2024-02-06T23:47:50.654+0100] {spark_submit.py:571} INFO - optional binary flag (STRING);
[2024-02-06T23:47:50.655+0100] {spark_submit.py:571} INFO - optional int64 id;
[2024-02-06T23:47:50.655+0100] {spark_submit.py:571} INFO - optional binary name (STRING);
[2024-02-06T23:47:50.655+0100] {spark_submit.py:571} INFO - }
[2024-02-06T23:47:50.656+0100] {spark_submit.py:571} INFO - optional binary code (STRING);
[2024-02-06T23:47:50.656+0100] {spark_submit.py:571} INFO - optional group currentSeason {
[2024-02-06T23:47:50.656+0100] {spark_submit.py:571} INFO - optional int64 currentMatchday;
[2024-02-06T23:47:50.656+0100] {spark_submit.py:571} INFO - optional binary endDate (STRING);
[2024-02-06T23:47:50.660+0100] {spark_submit.py:571} INFO - optional int64 id;
[2024-02-06T23:47:50.660+0100] {spark_submit.py:571} INFO - optional binary startDate (STRING);
[2024-02-06T23:47:50.660+0100] {spark_submit.py:571} INFO - optional binary winner (STRING);
[2024-02-06T23:47:50.661+0100] {spark_submit.py:571} INFO - }
[2024-02-06T23:47:50.661+0100] {spark_submit.py:571} INFO - optional binary emblem (STRING);
[2024-02-06T23:47:50.661+0100] {spark_submit.py:571} INFO - optional int64 id;
[2024-02-06T23:47:50.662+0100] {spark_submit.py:571} INFO - optional binary lastUpdated (STRING);
[2024-02-06T23:47:50.662+0100] {spark_submit.py:571} INFO - optional binary name (STRING);
[2024-02-06T23:47:50.662+0100] {spark_submit.py:571} INFO - optional group seasons (LIST) {
[2024-02-06T23:47:50.662+0100] {spark_submit.py:571} INFO - repeated group list {
[2024-02-06T23:47:50.662+0100] {spark_submit.py:571} INFO - optional group element {
[2024-02-06T23:47:50.663+0100] {spark_submit.py:571} INFO - optional int64 currentMatchday;
[2024-02-06T23:47:50.663+0100] {spark_submit.py:571} INFO - optional binary endDate (STRING);
[2024-02-06T23:47:50.663+0100] {spark_submit.py:571} INFO - optional int64 id;
[2024-02-06T23:47:50.663+0100] {spark_submit.py:571} INFO - optional binary startDate (STRING);
[2024-02-06T23:47:50.663+0100] {spark_submit.py:571} INFO - optional group winner {
[2024-02-06T23:47:50.664+0100] {spark_submit.py:571} INFO - optional binary address (STRING);
[2024-02-06T23:47:50.664+0100] {spark_submit.py:571} INFO - optional binary clubColors (STRING);
[2024-02-06T23:47:50.664+0100] {spark_submit.py:571} INFO - optional binary crest (STRING);
[2024-02-06T23:47:50.664+0100] {spark_submit.py:571} INFO - optional int64 founded;
[2024-02-06T23:47:50.664+0100] {spark_submit.py:571} INFO - optional int64 id;
[2024-02-06T23:47:50.665+0100] {spark_submit.py:571} INFO - optional binary lastUpdated (STRING);
[2024-02-06T23:47:50.665+0100] {spark_submit.py:571} INFO - optional binary name (STRING);
[2024-02-06T23:47:50.665+0100] {spark_submit.py:571} INFO - optional binary shortName (STRING);
[2024-02-06T23:47:50.665+0100] {spark_submit.py:571} INFO - optional binary tla (STRING);
[2024-02-06T23:47:50.665+0100] {spark_submit.py:571} INFO - optional binary venue (STRING);
[2024-02-06T23:47:50.666+0100] {spark_submit.py:571} INFO - optional binary website (STRING);
[2024-02-06T23:47:50.666+0100] {spark_submit.py:571} INFO - }
[2024-02-06T23:47:50.666+0100] {spark_submit.py:571} INFO - }
[2024-02-06T23:47:50.666+0100] {spark_submit.py:571} INFO - }
[2024-02-06T23:47:50.666+0100] {spark_submit.py:571} INFO - }
[2024-02-06T23:47:50.666+0100] {spark_submit.py:571} INFO - optional binary type (STRING);
[2024-02-06T23:47:50.667+0100] {spark_submit.py:571} INFO - }
[2024-02-06T23:47:50.667+0100] {spark_submit.py:571} INFO - 
[2024-02-06T23:47:50.667+0100] {spark_submit.py:571} INFO - 
[2024-02-06T23:47:51.372+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:51 INFO CodecPool: Got brand-new compressor [.snappy]
[2024-02-06T23:47:54.665+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:54 INFO FileScanRDD: Reading File path: hdfs://localhost:9000/datalake/raw/match/2024-01-02.json, range: 0-23381, partition values: [empty row]
[2024-02-06T23:47:55.201+0100] {spark_submit.py:571} INFO - 24/02/06 23:47:55 INFO CodeGenerator: Code generated in 355.642936 ms
[2024-02-06T23:48:01.911+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:01 INFO FileOutputCommitter: Saved output of task 'attempt_202402062347481827285405578526903_0001_m_000000_1' to hdfs://localhost:9000/datalake/formatted/2024-01-02/_temporary/0/task_202402062347481827285405578526903_0001_m_000000
[2024-02-06T23:48:01.926+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:01 INFO SparkHadoopMapRedUtil: attempt_202402062347481827285405578526903_0001_m_000000_1: Committed. Elapsed time: 176 ms.
[2024-02-06T23:48:02.025+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2541 bytes result sent to driver
[2024-02-06T23:48:02.050+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 12984 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-06T23:48:02.057+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO DAGScheduler: ResultStage 1 (parquet at FormattedData.scala:23) finished in 13.304 s
[2024-02-06T23:48:02.059+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-06T23:48:02.061+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-02-06T23:48:02.070+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-02-06T23:48:02.082+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO DAGScheduler: Job 1 finished: parquet at FormattedData.scala:23, took 13.396411 s
[2024-02-06T23:48:02.135+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO FileFormatWriter: Start to commit write Job 0c1a5bf6-d9a4-49de-a967-2ea7f7a74eef.
[2024-02-06T23:48:02.541+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO FileFormatWriter: Write Job 0c1a5bf6-d9a4-49de-a967-2ea7f7a74eef committed. Elapsed time: 425 ms.
[2024-02-06T23:48:02.556+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO FileFormatWriter: Finished processing stats for write job 0c1a5bf6-d9a4-49de-a967-2ea7f7a74eef.
[2024-02-06T23:48:02.779+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO SparkContext: Invoking stop() from shutdown hook
[2024-02-06T23:48:02.943+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:02 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
[2024-02-06T23:48:03.571+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-02-06T23:48:04.214+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:04 INFO MemoryStore: MemoryStore cleared
[2024-02-06T23:48:04.216+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:04 INFO BlockManager: BlockManager stopped
[2024-02-06T23:48:04.344+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:04 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-02-06T23:48:04.400+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-02-06T23:48:04.684+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:04 INFO SparkContext: Successfully stopped SparkContext
[2024-02-06T23:48:04.685+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:04 INFO ShutdownHookManager: Shutdown hook called
[2024-02-06T23:48:04.688+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c18d90c-71e3-42e2-b6e6-3d557ec5d5c9
[2024-02-06T23:48:04.715+0100] {spark_submit.py:571} INFO - 24/02/06 23:48:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-0dae434c-5696-4a7e-8bfa-b943ba462422
[2024-02-06T23:48:05.534+0100] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=Competition_dag, task_id=cleanMatch_task, execution_date=20240102T000000, start_date=20240206T224541, end_date=20240206T224805
[2024-02-06T23:48:05.868+0100] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-02-06T23:48:05.996+0100] {taskinstance.py:3280} INFO - 1 downstream tasks scheduled from follow-on schedule check
